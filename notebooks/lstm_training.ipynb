{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Nigerian Pidgin Next-Word Prediction: LSTM Model\n",
                "\n",
                "This notebook trains an LSTM language model for next-word prediction on Nigerian Pidgin text.\n",
                "\n",
                "**Data Sources:**\n",
                "- NaijaSenti PCM dataset (Hugging Face)\n",
                "- BBC Pidgin corpus\n",
                "\n",
                "**Run this in Google Colab with GPU runtime for faster training.**"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup & Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies\n",
                "!pip install datasets torch torchvision torchaudio --quiet\n",
                "\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from torch.nn.utils.rnn import pad_sequence\n",
                "\n",
                "from datasets import load_dataset\n",
                "from collections import Counter\n",
                "import numpy as np\n",
                "import re\n",
                "import math\n",
                "from typing import List, Tuple, Dict\n",
                "from tqdm.auto import tqdm\n",
                "\n",
                "# Check GPU\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load NaijaSenti PCM dataset\n",
                "print(\"Loading NaijaSenti PCM dataset...\")\n",
                "dataset = load_dataset(\"mteb/NaijaSenti\", \"pcm\")\n",
                "\n",
                "# Combine all splits\n",
                "all_texts = []\n",
                "for split in dataset.keys():\n",
                "    texts = [ex['text'] for ex in dataset[split]]\n",
                "    all_texts.extend(texts)\n",
                "    print(f\"  {split}: {len(texts):,} texts\")\n",
                "\n",
                "print(f\"\\nTotal: {len(all_texts):,} texts\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Optional: Clone and add BBC Pidgin corpus for more data\n",
                "# Uncomment below to include BBC Pidgin articles\n",
                "\n",
                "# !git clone https://github.com/keleog/bbc_pidgin_scraper.git\n",
                "# import csv\n",
                "# with open('bbc_pidgin_scraper/data/pidgin_corpus.csv', 'r', encoding='utf-8') as f:\n",
                "#     reader = csv.DictReader(f)\n",
                "#     for row in reader:\n",
                "#         headline = row.get('headline', '').strip()\n",
                "#         text = row.get('text', '').strip()\n",
                "#         if headline and text:\n",
                "#             all_texts.append(f\"{headline}. {text}\")\n",
                "# print(f\"With BBC Pidgin: {len(all_texts):,} texts\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Preprocessing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def clean_text(text: str) -> str:\n",
                "    \"\"\"Clean text while preserving Nigerian Pidgin features.\"\"\"\n",
                "    text = text.lower()\n",
                "    text = re.sub(r'https?://\\S+', '', text)  # Remove URLs\n",
                "    text = re.sub(r'www\\.\\S+', '', text)\n",
                "    text = re.sub(r'@\\w+', '', text)  # Remove @usernames\n",
                "    text = re.sub(r'#(\\w+)', r'\\1', text)  # Remove # but keep word\n",
                "    text = re.sub(r'\\s+', ' ', text)  # Normalize whitespace\n",
                "    return text.strip()\n",
                "\n",
                "def tokenize(text: str) -> List[str]:\n",
                "    \"\"\"Simple word tokenization.\"\"\"\n",
                "    # Split on whitespace and punctuation\n",
                "    tokens = re.findall(r\"[\\w']+|[.,!?;:]\", text)\n",
                "    return tokens\n",
                "\n",
                "# Process all texts\n",
                "print(\"Preprocessing...\")\n",
                "processed_sentences = []\n",
                "for text in tqdm(all_texts):\n",
                "    cleaned = clean_text(text)\n",
                "    tokens = tokenize(cleaned)\n",
                "    if len(tokens) >= 3:  # Need at least 3 tokens for meaningful sequences\n",
                "        processed_sentences.append(tokens)\n",
                "\n",
                "print(f\"Processed {len(processed_sentences):,} sentences\")\n",
                "print(f\"Sample: {processed_sentences[0][:10]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Build Vocabulary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Special tokens\n",
                "PAD_TOKEN = '<PAD>'\n",
                "UNK_TOKEN = '<UNK>'\n",
                "SOS_TOKEN = '<SOS>'  # Start of sequence\n",
                "EOS_TOKEN = '<EOS>'  # End of sequence\n",
                "\n",
                "# Count word frequencies\n",
                "word_counts = Counter()\n",
                "for sentence in processed_sentences:\n",
                "    word_counts.update(sentence)\n",
                "\n",
                "print(f\"Total unique words: {len(word_counts):,}\")\n",
                "print(f\"Top 20 words: {word_counts.most_common(20)}\")\n",
                "\n",
                "# Build vocabulary (keep words appearing >= 2 times)\n",
                "MIN_FREQ = 2\n",
                "vocab = [PAD_TOKEN, UNK_TOKEN, SOS_TOKEN, EOS_TOKEN]\n",
                "vocab += [word for word, count in word_counts.most_common() if count >= MIN_FREQ]\n",
                "\n",
                "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
                "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
                "\n",
                "VOCAB_SIZE = len(vocab)\n",
                "print(f\"\\nVocabulary size: {VOCAB_SIZE:,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Create Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class NextWordDataset(Dataset):\n",
                "    \"\"\"\n",
                "    Dataset for next-word prediction.\n",
                "    Each sample: (input_sequence, target_word)\n",
                "    \"\"\"\n",
                "    def __init__(self, sentences: List[List[str]], word_to_idx: Dict, seq_length: int = 10):\n",
                "        self.samples = []\n",
                "        self.seq_length = seq_length\n",
                "        unk_idx = word_to_idx[UNK_TOKEN]\n",
                "        \n",
                "        for sentence in sentences:\n",
                "            # Convert to indices\n",
                "            indices = [word_to_idx.get(w, unk_idx) for w in sentence]\n",
                "            \n",
                "            # Create samples: sliding window over sentence\n",
                "            for i in range(1, len(indices)):\n",
                "                # Input: previous tokens (up to seq_length)\n",
                "                start_idx = max(0, i - seq_length)\n",
                "                input_seq = indices[start_idx:i]\n",
                "                # Target: current token\n",
                "                target = indices[i]\n",
                "                self.samples.append((input_seq, target))\n",
                "    \n",
                "    def __len__(self):\n",
                "        return len(self.samples)\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        input_seq, target = self.samples[idx]\n",
                "        return torch.tensor(input_seq, dtype=torch.long), torch.tensor(target, dtype=torch.long)\n",
                "\n",
                "def collate_fn(batch):\n",
                "    \"\"\"Pad sequences to same length.\"\"\"\n",
                "    inputs, targets = zip(*batch)\n",
                "    # Pad sequences\n",
                "    inputs_padded = pad_sequence(inputs, batch_first=True, padding_value=0)\n",
                "    targets = torch.stack(targets)\n",
                "    return inputs_padded, targets"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create train/val split\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "train_sentences, val_sentences = train_test_split(\n",
                "    processed_sentences, test_size=0.1, random_state=42\n",
                ")\n",
                "\n",
                "SEQ_LENGTH = 15\n",
                "BATCH_SIZE = 128\n",
                "\n",
                "train_dataset = NextWordDataset(train_sentences, word_to_idx, seq_length=SEQ_LENGTH)\n",
                "val_dataset = NextWordDataset(val_sentences, word_to_idx, seq_length=SEQ_LENGTH)\n",
                "\n",
                "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
                "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
                "\n",
                "print(f\"Train samples: {len(train_dataset):,}\")\n",
                "print(f\"Val samples: {len(val_dataset):,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. LSTM Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class LSTMLanguageModel(nn.Module):\n",
                "    \"\"\"\n",
                "    LSTM-based language model for next-word prediction.\n",
                "    \n",
                "    Architecture:\n",
                "    - Embedding layer\n",
                "    - LSTM layer(s)\n",
                "    - Dropout\n",
                "    - Linear output layer\n",
                "    \"\"\"\n",
                "    def __init__(\n",
                "        self, \n",
                "        vocab_size: int, \n",
                "        embed_dim: int = 256, \n",
                "        hidden_dim: int = 512, \n",
                "        num_layers: int = 2, \n",
                "        dropout: float = 0.3\n",
                "    ):\n",
                "        super().__init__()\n",
                "        \n",
                "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
                "        self.lstm = nn.LSTM(\n",
                "            embed_dim, \n",
                "            hidden_dim, \n",
                "            num_layers=num_layers, \n",
                "            batch_first=True,\n",
                "            dropout=dropout if num_layers > 1 else 0\n",
                "        )\n",
                "        self.dropout = nn.Dropout(dropout)\n",
                "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
                "        \n",
                "        self.hidden_dim = hidden_dim\n",
                "        self.num_layers = num_layers\n",
                "    \n",
                "    def forward(self, x):\n",
                "        # x: (batch, seq_len)\n",
                "        embedded = self.embedding(x)  # (batch, seq_len, embed_dim)\n",
                "        lstm_out, _ = self.lstm(embedded)  # (batch, seq_len, hidden_dim)\n",
                "        # Take last output\n",
                "        last_out = lstm_out[:, -1, :]  # (batch, hidden_dim)\n",
                "        out = self.dropout(last_out)\n",
                "        logits = self.fc(out)  # (batch, vocab_size)\n",
                "        return logits\n",
                "    \n",
                "    def predict_next_words(self, context: str, word_to_idx: Dict, idx_to_word: Dict, top_k: int = 5):\n",
                "        \"\"\"Predict next words given context string.\"\"\"\n",
                "        self.eval()\n",
                "        \n",
                "        # Tokenize and convert to indices\n",
                "        tokens = tokenize(clean_text(context))\n",
                "        unk_idx = word_to_idx[UNK_TOKEN]\n",
                "        indices = [word_to_idx.get(t, unk_idx) for t in tokens]\n",
                "        \n",
                "        if not indices:\n",
                "            return []\n",
                "        \n",
                "        # Create input tensor\n",
                "        x = torch.tensor([indices], dtype=torch.long).to(next(self.parameters()).device)\n",
                "        \n",
                "        with torch.no_grad():\n",
                "            logits = self(x)\n",
                "            probs = torch.softmax(logits, dim=-1)\n",
                "            \n",
                "        # Get top-k predictions\n",
                "        top_probs, top_indices = torch.topk(probs[0], top_k)\n",
                "        \n",
                "        predictions = []\n",
                "        for prob, idx in zip(top_probs.cpu().numpy(), top_indices.cpu().numpy()):\n",
                "            word = idx_to_word.get(idx, UNK_TOKEN)\n",
                "            if word not in [PAD_TOKEN, UNK_TOKEN, SOS_TOKEN, EOS_TOKEN]:\n",
                "                predictions.append((word, float(prob)))\n",
                "        \n",
                "        return predictions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Hyperparameters\n",
                "EMBED_DIM = 256\n",
                "HIDDEN_DIM = 512\n",
                "NUM_LAYERS = 2\n",
                "DROPOUT = 0.3\n",
                "LEARNING_RATE = 0.001\n",
                "NUM_EPOCHS = 10\n",
                "\n",
                "# Initialize model\n",
                "model = LSTMLanguageModel(\n",
                "    vocab_size=VOCAB_SIZE,\n",
                "    embed_dim=EMBED_DIM,\n",
                "    hidden_dim=HIDDEN_DIM,\n",
                "    num_layers=NUM_LAYERS,\n",
                "    dropout=DROPOUT\n",
                ").to(device)\n",
                "\n",
                "# Count parameters\n",
                "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
                "print(f\"Model parameters: {num_params:,}\")\n",
                "print(model)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding\n",
                "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
                "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
                "\n",
                "def train_epoch(model, loader, criterion, optimizer):\n",
                "    model.train()\n",
                "    total_loss = 0\n",
                "    for inputs, targets in tqdm(loader, desc=\"Training\"):\n",
                "        inputs, targets = inputs.to(device), targets.to(device)\n",
                "        \n",
                "        optimizer.zero_grad()\n",
                "        outputs = model(inputs)\n",
                "        loss = criterion(outputs, targets)\n",
                "        loss.backward()\n",
                "        \n",
                "        # Gradient clipping\n",
                "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
                "        \n",
                "        optimizer.step()\n",
                "        total_loss += loss.item()\n",
                "    \n",
                "    return total_loss / len(loader)\n",
                "\n",
                "def evaluate(model, loader, criterion):\n",
                "    model.eval()\n",
                "    total_loss = 0\n",
                "    with torch.no_grad():\n",
                "        for inputs, targets in loader:\n",
                "            inputs, targets = inputs.to(device), targets.to(device)\n",
                "            outputs = model(inputs)\n",
                "            loss = criterion(outputs, targets)\n",
                "            total_loss += loss.item()\n",
                "    \n",
                "    avg_loss = total_loss / len(loader)\n",
                "    perplexity = math.exp(avg_loss)\n",
                "    return avg_loss, perplexity"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Training loop\n",
                "print(\"Starting training...\")\n",
                "best_val_loss = float('inf')\n",
                "\n",
                "for epoch in range(NUM_EPOCHS):\n",
                "    train_loss = train_epoch(model, train_loader, criterion, optimizer)\n",
                "    val_loss, val_ppl = evaluate(model, val_loader, criterion)\n",
                "    \n",
                "    scheduler.step(val_loss)\n",
                "    \n",
                "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
                "    print(f\"  Train Loss: {train_loss:.4f}\")\n",
                "    print(f\"  Val Loss: {val_loss:.4f} | Val Perplexity: {val_ppl:.2f}\")\n",
                "    \n",
                "    # Save best model\n",
                "    if val_loss < best_val_loss:\n",
                "        best_val_loss = val_loss\n",
                "        torch.save({\n",
                "            'model_state_dict': model.state_dict(),\n",
                "            'word_to_idx': word_to_idx,\n",
                "            'idx_to_word': idx_to_word,\n",
                "            'vocab_size': VOCAB_SIZE,\n",
                "        }, 'lstm_pidgin_model.pt')\n",
                "        print(\"  Saved best model!\")\n",
                "\n",
                "print(\"\\nTraining complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Inference - Next Word Prediction"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test predictions\n",
                "test_contexts = [\n",
                "    \"i dey\",\n",
                "    \"wetin you\",\n",
                "    \"na the\",\n",
                "    \"how far\",\n",
                "    \"e don\",\n",
                "    \"you no\",\n",
                "    \"make we\",\n",
                "    \"dem dey\",\n",
                "]\n",
                "\n",
                "print(\"Next-Word Predictions (LSTM):\")\n",
                "print(\"=\" * 50)\n",
                "for context in test_contexts:\n",
                "    predictions = model.predict_next_words(context, word_to_idx, idx_to_word, top_k=5)\n",
                "    pred_str = \", \".join([f\"{w} ({p:.2%})\" for w, p in predictions])\n",
                "    print(f\"'{context}' â†’ {pred_str}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Interactive prediction\n",
                "def predict_interactive():\n",
                "    while True:\n",
                "        context = input(\"\\nEnter context (or 'quit'): \")\n",
                "        if context.lower() == 'quit':\n",
                "            break\n",
                "        predictions = model.predict_next_words(context, word_to_idx, idx_to_word, top_k=5)\n",
                "        print(\"Predictions:\")\n",
                "        for word, prob in predictions:\n",
                "            print(f\"  {word}: {prob:.2%}\")\n",
                "\n",
                "# Uncomment to use:\n",
                "# predict_interactive()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Save Model for Download"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Download trained model\n",
                "from google.colab import files\n",
                "files.download('lstm_pidgin_model.pt')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Model Comparison Notes\n",
                "\n",
                "| Model | Context Window | Perplexity | Notes |\n",
                "|-------|---------------|------------|-------|\n",
                "| Trigram | 2 words | Higher | Fast, interpretable |\n",
                "| LSTM | Variable (15) | Lower | Captures longer patterns |\n",
                "| Transformer | Full sequence | Lowest | Best quality, slower |\n",
                "\n",
                "**Next Steps:**\n",
                "- Try Transformer model for comparison\n",
                "- Use subword tokenization (BPE) for better OOV handling\n",
                "- Fine-tune on more domain-specific data"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        },
        "accelerator": "GPU"
    },
    "nbformat": 4,
    "nbformat_minor": 4
}